# Prompts

This directory contains various prompts and evaluations used for testing and benchmarking language models. It's focused on poem generation tasks and includes prompts for generating poems as well as evaluating their quality using LLM-Judge.

## LLM-Judge evaluations

There are two prompts to evaluate the output of the target LLMs with help of a Judge LLM:
- Use [`Judge.md`](./Judge.md) to evaluate poem generation quality using LLM-Judge.
In this case the LLM knows the author of the poem and can use that information to better evaluate the generated poems.
- Use [`BlindJudge.md`](./BlindJudge.md) to evaluate poem generation quality using LLM-Judge without providing the author information. This helps us to reduce potential bias in the evaluation.

### How to use

Copy the output generated by the target LLM for the poem generation task and paste it into the appropriate prompt file (either `Judge.md` and `BlindJudge.md`).

Then, use a Judge LLM to process the prompt and generate the evaluation results.

If you're using ChatGPT, use a separated chat on anonymous tab for each evaluation to avoid context contamination.

When pasting the evaluation results, make sure to keep the original format for consistency.
Also make sure to add references of the evaluated LLM and the Judge LLM used for the evaluation.
